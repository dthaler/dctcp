<?xml version="1.0"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd"[
  <!ENTITY rfc0793 SYSTEM "http://xml2rfc.tools.ietf.org/public/rfc/bibxml/reference.RFC.0793.xml">
  <!ENTITY rfc2018 SYSTEM "http://xml2rfc.tools.ietf.org/public/rfc/bibxml/reference.RFC.2018.xml">
  <!ENTITY rfc2119 SYSTEM "http://xml2rfc.tools.ietf.org/public/rfc/bibxml/reference.RFC.2119.xml">
  <!ENTITY rfc3168 SYSTEM "http://xml2rfc.tools.ietf.org/public/rfc/bibxml/reference.RFC.3168.xml">
  <!ENTITY rfc5681 SYSTEM "http://xml2rfc.tools.ietf.org/public/rfc/bibxml/reference.RFC.5681.xml">
]>
<?rfc toc='yes' ?>
<?rfc symrefs='yes' ?>
<?rfc sortrefs='no'?>
<?rfc compact='yes'?>
<?rfc comments="yes"?>
<?rfc inline="yes" ?>
<!-- <?rfc-ext parse-xml-in-artwork='yes' ?> -->
<!-- <?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?> -->

<rfc docName="draft-bensley-tcpm-dctcp-03" ipr="trust200902" category="info">
  <front>
    <title abbrev='DCTCP'>Microsoft's&nbsp;Datacenter&nbsp;TCP&nbsp;(DCTCP): TCP&nbsp;Congestion&nbsp;Control&nbsp;for&nbsp;Datacenters</title>
    <author initials='S.' surname='Bensley' fullname='Stephen Bensley'>
      <organization>Microsoft</organization>
      <address>
        <postal>
          <street>One Microsoft Way</street>
          <city>Redmond</city>
          <region>WA</region>
          <code>98052</code>
          <country>USA</country>
        </postal>
        <phone>+1 425 703 5570</phone>
        <email>sbens@microsoft.com</email>
      </address>
    </author>
    <author initials='L.' surname='Eggert' fullname='Lars Eggert'>
      <organization>NetApp</organization>
      <address>
        <postal>
          <street>Sonnenallee 1</street>
          <city>Kirchheim</city>
          <code>85551</code>
          <country>Germany</country>
        </postal>
        <phone>+49 151 120 55791</phone>
        <email>lars@netapp.com</email>
        <uri>http://eggert.org/</uri>
      </address>
    </author>
    <author initials='D.' surname='Thaler' fullname='Dave Thaler'>
      <organization>Microsoft</organization>
      <address>
        <phone>+1 425 703 8835</phone>
        <email>dthaler@microsoft.com</email>
      </address>
    </author>
    <date/>
    <area>Transport</area>
    <keyword>TCP</keyword>
    <keyword>ECN</keyword>
    <keyword>DCTCP</keyword>
    <keyword>congestion control</keyword>
    <abstract>
      <t> This memo describes Datacenter TCP (DCTCP), an improvement to TCP congestion control for datacenter traffic,
      as implemented in Windows Server 2012. DCTCP
      enhances Explicit Congestion Notification (ECN) processing to estimate the fraction of bytes that encounter congestion,
      rather than simply detecting that some congestion has occurred. DCTCP then scales the TCP congestion window based on this
      estimate. This method achieves high burst tolerance, low latency, and high throughput with shallow-buffered switches.
      </t>
    </abstract>
  </front>

  <middle>
    <section title='Introduction'>
      <t> Large datacenters necessarily need a large number of network switches to interconnect the servers in the datacenter.
      Therefore, a datacenter can greatly reduce its capital expenditure by leveraging low cost switches. However, low cost
      switches tend to have limited queue capacities and thus are more susceptible to packet loss due to congestion.
      </t>
      <t> Network traffic in the datacenter is often a mix of short and long flows, where the short flows require low latency and
      the long flows require high throughput. Datacenters also experience incast bursts, where many endpoints send traffic to a
      single server at the same time. For example, this is a natural consequence of MapReduce algorithms. The worker nodes
      complete at approximately the same time, and all reply to the master node concurrently.
      </t>
      <t> These factors place some conflicting demands on the queue occupancy of a switch:
        <list style='symbols'>
          <t> The queue must be short enough that it does not impose excessive latency on short flows.
          </t>
          <t> The queue must be long enough to buffer sufficient data for the long flows to saturate the path bandwidth.
          </t>
          <t> The queue must be short enough to absorb incast bursts without excessive packet loss.
          </t>
        </list>
      </t>
      <t> Standard TCP congestion control <xref target="RFC5681"/> relies on segment loss to detect congestion. This does not meet
      the demands described above. First, the short flows will start to experience unacceptable latencies before packet loss
      occurs. Second, by the time TCP congestion control kicks in on the sender, most of the incast burst has already been dropped.
      </t>
      <t> <xref target="RFC3168"/> describes a mechanism for using Explicit Congestion Notification (ECN) from the switch for early
      detection of congestion, rather than waiting for segment loss to occur. However, this method only detects the presence of
      congestion, not the extent. In the presence of mild congestion, it reduces the TCP congestion window too aggressively and
      unnecessarily affects the throughput of long flows.
      </t>
      <t> Datacenter TCP (DCTCP) enhances ECN processing to estimate the fraction of bytes that
      encounter congestion, rather than simply detecting that some congestion has occurred. DCTCP then scales the TCP congestion
      window based on this estimate. This method achieves high burst tolerance, low latency, and high throughput with
      shallow-buffered switches.
      </t>
      <t>
      This document describes DCTCP as implemented in Microsoft Windows Server 2012.
      Since publication of the first versions of this document, the Linux <xref target="LINUX"/>
      and FreeBSD <xref target="FREEBSD"/> operating systems have also implemented support for DCTCP in a way
      that is believed to follow this document.
      </t>
    </section>

    <section title="Terminology" anchor="term">
      <t>
      The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in <xref target="RFC2119"/>.
      </t>
    </section>

    <section title='DCTCP Algorithm'>
      <t> There are three components involved in the DCTCP algorithm:
        <list style='symbols'>
          <t> The switch (or other intermediate device on the network) detects congestion and sets the Congestion Encountered (CE)
              codepoint in the IP header.
          </t>
          <t> The receiver echoes the congestion information back to the sender using the ECN-Echo (ECE) flag in the TCP header.
          </t>
          <t> The sender reacts to the congestion indication by reducing the TCP congestion window (cwnd).
          </t>
        </list>
      </t>

      <section title='Marking Congestion on the Switch'>
        <t> The switch indicates congestion to the end nodes by setting the CE codepoint in the IP header as specified in Section 5 of <xref target="RFC3168"/>.
        For example, the switch may be configured with a congestion threshold. When a packet arrives at the switch and
        its queue length is greater than the congestion threshold, the switch sets the CE codepoint in the packet.
        However, the actual algorithm for marking congestion is an implementation detail of the switch and will generally not be
        known to the sender and receiver.
        </t>
      </section>

      <section title='Echoing Congestion Information on the Receiver'>
        <t> According to Section 6.1.3 of <xref target="RFC3168"/>, the
            receiver sets the ECE flag if any of the packets being acknowledged had the CE code point
        set. The receiver then continues to set the ECE flag until it receives a packet with the Congestion Window Reduced (CWR)
        flag set. However, the DCTCP algorithm requires more detailed congestion information. In particular, the sender must be
        able to determine the number of sent bytes that encountered congestion. Thus, the scheme described in <xref target="RFC3168"/> does not
        suffice.
        </t>
        <t> One possible solution is to ACK every packet and set the ECE flag in the ACK if and only if the CE code point was set
        in the packet being acknowledged. However, this prevents the use of delayed ACKs, which are an important performance
        optimization in datacenters.
        </t>
        <t>Instead, DCTCP introduces a new Boolean TCP state variable, DCTCP Congestion Encountered
        (DCTCP.CE), which is initialized to false and stored in the
        Transmission Control Block (TCB). When sending an ACK, the ECE flag MUST be set if and only if
        DCTCP.CE is true. When receiving packets, the CE codepoint MUST be processed as follows:
          <list style='numbers'>
           <t> If the CE codepoint is set and DCTCP.CE is false, send an ACK for any previously unacknowledged packets and set
           DCTCP.CE to true.
           </t>
           <t> If the CE codepoint is not set and DCTCP.CE is true, send an ACK for any previously unacknowledged packets and set
           DCTCP.CE to false.
           </t>
           <t> Otherwise, ignore the CE codepoint.
           </t>
           </list>
        </t>
      </section>
      <section title='Processing Congestion Indications on the Sender' anchor='senderprocessing'>
        <t> The sender estimates the fraction of sent bytes that encountered congestion. The current estimate is stored in a new
        TCP state variable, DCTCP.Alpha, which is initialized to 1 and MUST be updated as follows:
          <list>
            <t> DCTCP.Alpha = DCTCP.Alpha * (1 - g) + g * M </t>
          </list>
        </t>
        <t> where
          <list style='symbols'>
            <t> g is the estimation gain, a real number between 0 and 1. The selection of g is left to the implementation.
            See <xref target="implissues"/> for further considerations.
            </t>
            <t> M is the fraction of sent bytes that encountered congestion during the previous observation window, where the
            observation window is chosen to be approximately the Round Trip Time (RTT). In particular, an observation window
            ends when all the sent bytes in flight at the beginning of the window have been acknowledged.
            </t>
          </list>
        </t>
        <t> In order to update DCTCP.Alpha, the TCP state variables defined in <xref target="RFC0793"/> are used, and
        three additional TCP state variables are introduced:
          <list style='symbols'>
            <t> DCTCP.WindowEnd: The TCP sequence number threshold for beginning a new observation window; initialized to
            SND.UNA.
            </t>
            <t> DCTCP.BytesSent: The number of bytes sent during the current observation window; initialized to zero.
            </t>
            <t> DCTCP.BytesMarked: The number of bytes sent during the current observation window that encountered congestion; initialized
            to zero.
            </t>
          </list>
        </t>
        <t> The congestion estimator on the sender MUST process acceptable ACKs as follows:
          <list style='numbers'>
            <t> Compute the bytes acknowledged (TCP SACK options <xref target="RFC2018"/> are ignored):
              <list style='none'>
                <t> BytesAcked = SEG.ACK - SND.UNA </t>
              </list>
            </t>
            <t> Update the bytes sent:
              <list style='none'>
                <t> DCTCP.BytesSent += BytesAcked </t>
              </list>
            </t>
            <t> If the ECE flag is set, update the bytes marked:
              <list style='none'>
                <t> DCTCP.BytesMarked += BytesAcked </t>
              </list>
            </t>
            <t> If the sequence number is less than or equal to DCTCP.WindowEnd, then stop processing. Otherwise,
            the end of the observation window was reached, so proceed to update the congestion estimate as follows:
            </t>
            <t> Compute the congestion level for the current observation window:
              <list style='none'>
                <t> M = DCTCP.BytesMarked / DCTCP.BytesSent </t>
              </list>
            </t>
            <t> Update the congestion estimate:
              <list style='none'>
                <t> DCTCP.Alpha = DCTCP.Alpha * (1 - g) + g * M </t>
              </list>
            </t>
            <t> Determine the end of the next observation window:
              <list style='none'>
                <t> DCTCP.WindowEnd = SND.NXT </t>
              </list>
            </t>
            <t> Reset the byte counters:
              <list style='none'>
                <t> DCTCP.BytesSent = DCTCP.BytesMarked = 0 </t>
              </list>
            </t>
          </list>
        </t>
        <t> Rather than always halving the congestion window as described in <xref target="RFC3168"/>, when the sender receives an
        indication of congestion, the sender MUST update cwnd as follows:
          <list>
            <t> cwnd = cwnd * (1 - DCTCP.Alpha / 2)
            </t>
          </list>
        </t>
        <t> Thus, when no sent byte experienced congestion, DCTCP.Alpha equals zero, and cwnd is left unchanged. When all sent
        bytes experienced congestion, DCTCP.Alpha equals one, and cwnd is reduced by half. Lower levels of congestion will result
        in correspondingly smaller reductions to cwnd.
        </t>
        <t> Just as specified in <xref target="RFC3168"/>, TCP should not react to congestion indications more than once every
        window of data. The setting of the "Congestion Window Reduced" (CWR) bit is also exactly as per <xref target="RFC3168"/>.
        </t>
      </section>
    </section>

    <section title='Implementation Issues' anchor="implissues">
      <t> As noted in <xref target='senderprocessing' />, the implementation must choose a suitable estimation gain.
      <xref target='DCTCP10' /> provides a theoretical basis for selecting the gain. However, it may be more practical to use
      experimentation to select a suitable gain for a particular network and workload. The Microsoft implementation of DCTCP
      in Windows Server 2012 uses a fixed estimation gain of 1/16.
      </t>
      <t> The implementation must also decide when to use DCTCP. Datacenter servers may need to communicate with endpoints outside
      the datacenter, where DCTCP is unsuitable or unsupported. Thus, a global configuration setting to enable DCTCP will
      generally not suffice. DCTCP may be configured based on the IP address of the remote endpoint. Microsoft Windows Server 2012
      also supports automatic selection of DCTCP if the estimated RTT is less than 10 msec, under the assumption that if the RTT
      is low, then the two endpoints are likely on the same datacenter network.
      </t>
    </section>

    <section title='Deployment Issues'>
      <t> Since DCTCP relies on congestion marking by the switch, DCTCP can only be deployed in datacenters where the network
      infrastructure supports ECN. The switches may also support configuration of the congestion threshold used for marking.
      <xref target='DCTCP10' /> provides a theoretical basis for selecting the congestion threshold, but as with estimation gain,
      it may be more practical to rely on experimentation or simply to use the default configuration of the device.
      </t>
      <t> DCTCP requires changes on both the sender and the receiver, so both endpoints must support DCTCP. Furthermore, DCTCP
      provides no mechanism for negotiating its use, so both endpoints must be configured through some out-of-band mechanism to
      use DCTCP. A variant of DCTCP that can be deployed unilaterally and only requires standard ECN behavior has been described
      in <xref target="ODCTCP"/>, but requires additional experimental evaluation.
      </t>
    </section>

    <section title='Known Issues'>
      <t> DCTCP relies on the sender's ability to reconstruct the stream of CE codepoints received by the remote endpoint. To
      accomplish this, DCTCP avoids using a single ACK packet to acknowledge segments received both with and without the CE
      codepoint set. However, if an ACK packet is dropped, it's possible that a subsequent ACK will indeed acknowledge a mix of
      CE and non-CE segments. This will, of course, result in a less accurate congestion estimate. There are some potential
      mitigations:
        <list style="symbols">
          <t> Even with a degraded congestion estimate, DCTCP may still perform better than <xref target="RFC3168"/>.
          </t>
          <t> If the estimation gain is small relative to the packet loss rate, the estimate may not be degraded much.
          </t>
          <t> If packet losses mostly occur under heavy congestion, most drops will occur during an unbroken string of CE packets,
          and the estimate will be unaffected.
          </t>
        </list>
      However, the affect of packet drops on DCTCP under real world conditions has not been analyzed.
      </t>
      <t> DCTCP provides no mechanism for negotiating its use. Thus, there is additional management and configuration overhead
      required to ensure that DCTCP is not used with non-DCTCP endpoints. The affect of using DCTCP with a standard ECN endpoint
      has been analyzed in <xref target="ODCTCP"/>. Furthermore, it's possible that other implementations may also modify
      <xref target="RFC3168"/> behavior without negotiation, causing further interoperability issues.
      </t>
      <t> Much like standard TCP, DCTCP is biased against flows with longer RTTs. A method for improving the fairness of DCTCP
      has been proposed in <xref target="ADCTCP"/>, but requires additional experimental evaluation.
      </t>
    </section>

    <section title='Security Considerations'>
      <t> DCTCP enhances ECN and thus inherits the security considerations discussed in <xref target="RFC3168"/>. The processing
      changes introduced by DCTCP do not exacerbate these considerations or introduce new ones. In particular, with either
      algorithm, the network infrastructure or the remote endpoint can falsely report congestion and thus cause the sender to
      reduce cwnd. However, this is no worse than what can be achieved by simply dropping packets.
      </t>
    </section>

    <section title='IANA Considerations'>
      <t> This document has no actions for IANA.
      </t>
    </section>

    <section title='Acknowledgements'>
      <t> The DCTCP algorithm was originally proposed and analyzed in <xref target="DCTCP10"/> by Mohammad Alizadeh,
          Albert Greenberg, Dave Maltz, Jitu Padhye, Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and
          Murari Sridharan.
      </t>
    </section>
  </middle>

  <back>
    <references title='Normative References'>
      &rfc0793;
      &rfc2018;
      &rfc2119;
      &rfc3168;
    </references>

    <references title='Informative References'>
      &rfc5681;
      <reference anchor='DCTCP10' target='http://www.sigcomm.org/ccr/papers/2010/October/1851275.1851192/'>
        <front>
          <title>Data Center TCP (DCTCP)</title>
          <author initials="M." surname="Alizadeh" fullname="Mohammad Alizadeh">
            <organization abbrev="Stanford"> Stanford University </organization>
            <address>
              <email> alizade@stanford.edu </email>
            </address>
          </author>
          <author initials="A." surname="Greenberg" fullname="Albert Greenberg">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> albert@microsoft.com </email>
            </address>
          </author>
          <author initials="D.A." surname="Maltz" fullname="David A. Maltz">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> dmaltz@microsoft.com </email>
            </address>
          </author>
          <author initials="J." surname="Padhye" fullname="Jitendra Padhye">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> padhye@microsoft.com </email>
            </address>
          </author>
          <author initials="P." surname="Patel" fullname="Parveen Patel">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> parveenp@microsoft.com </email>
            </address>
          </author>
          <author initials="B." surname="Prabhakar" fullname="Balaji Prabhakar">
            <organization abbrev="Stanford"> Stanford University </organization>
            <address>
              <email> balaji@stanford.edu </email>
            </address>
          </author>
          <author initials="S." surname="Sengupta" fullname="Sudipta Sengupta">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> sudipta@microsoft.com </email>
            </address>
          </author>
          <author initials="M." surname="Sridharan" fullname="Murari Sridharan">
            <organization abbrev="MSR"> Microsoft Research </organization>
            <address>
              <email> muraris@microsoft.com </email>
            </address>
          </author>
          <date year="2010" month="December"/>
        </front>
      </reference>
      <reference anchor="ODCTCP" target="http://eggert.org/students/kato-thesis.pdf">
        <front>
          <title>Improving Transmission Performance with
One-Sided Datacenter TCP</title>
          <author initials="M." surname="Kato">
            <organization />
          </author>
          <date year="2014"/>
        </front>
        <seriesInfo name="M.S. Thesis," value="Keio University"/>
      </reference>
      <reference anchor="ADCTCP" target="http://simula.stanford.edu/~alizade/Site/DCTCP_files/dctcp_analysis-full.pdf">
        <front>
          <title> Analysis of DCTCP: Stability, Convergence, and Fairness </title>
          <author initials="M." surname="Alizadeh" fullname="Mohammad Alizadeh">
            <organization abbrev="Stanford"> Stanford University </organization>
            <address>
              <email> alizade@stanford.edu </email>
            </address>
          </author>
          <author initials="A." surname="Javanmard" fullname="Adel Javanmard">
            <organization abbrev="Stanford"> Stanford University </organization>
            <address>
              <email> adelj@stanford.edu </email>
            </address>
          </author>
          <author initials="B." surname="Prabhakar" fullname="Balaji Prabhakar">
            <organization abbrev="Stanford"> Stanford University </organization>
            <address>
              <email> balaji@stanford.edu </email>
            </address>
          </author>
          <date year="2011" month="June"/>
        </front>
      </reference>
      <reference anchor="LINUX" target="https://git.kernel.org/cgit/linux/kernel/git/davem/net-next.git/commit/?id=e3118e8359bb7c59555aca60c725106e6d78c5ce">
        <front>
          <title>Linux DCTCP patch</title>
          <author initials="D." surname="Borkmann">
            <organization />
          </author>
          <author initials="F." surname="Westphal">
            <organization />
          </author>
          <date year="2014"/>
        </front>
      </reference>
      <reference anchor="FREEBSD" target="https://github.com/freebsd/freebsd/commit/8ad879445281027858a7fa706d13e458095b595f">
        <front>
          <title>DCTCP (Data Center TCP) implementation</title>
          <author initials="M." surname="Kato">
            <organization />
          </author>
          <author initials="H." surname="Panchasara">
            <organization />
          </author>
          <date year="2015"/>
        </front>
      </reference>
    </references>
  </back>
</rfc>
